# ros2-camera-onnx-nav

画像を入力とした行動モデルによる移動ロボットの自律走行を目的とする

## 構成

- `unity-nav-ws/` : Unity 環境で ROS2-For-Unity を使用して行動モデルの推論を行うワークスペース
- `real-nav-ws/` : 実機移動ロボットへ行動モデルを適用して自律走行させるワークスペース

## 開発環境
- ROS2 jazzy
- Python 3.12

## 仮想環境
- ワークスペース単位で `.venv` を作成して管理
- `python -m colcon build` / `ros2 run` 実行前に `.venv` を有効化

## 想定する行動モデル

- Unity ML-Agents を用いて学習した深層強化学習モデル (ONNX)
- 入力
  - カメラ画像
    - 複数枚でも可
    - 基本的には４値化環境で学習させる想定
  - 目的地までの距離(m)
  - 目的地までの角度
    - -180 ~ 180(deg)
- 出力
  - 速度$v$[m/s]
    - -1<= $v$ <=1
    - **0以下は0にクリップ**（安全のため後進は禁止）
  - 角速度$\omega$[rad/s]
    - -1<= $\omega$ <=1

## real-nav-ws

### パッケージ

- `img_seg_pkg`
  - セマンティックセグメンテーション + fine-tuning した YOLO-seg を用いて、推論結果を 4値に整理

### 4値化のクラス・色

| クラス | 色(BGR) |
| --- | --- |
| 走行可能領域（床） | 緑（0, 255, 0） |
| 同方向歩行者 | 青（255, 0, 0） |
| 同方向以外歩行者 | 赤（0, 0, 255） |
| その他 | シアン（255, 255, 0） |

## unity-nav-ws

- Unity環境の行動モデルをUnity ML-Agentsの機能ではなく、ROS2環境で推論させる
- Unity - ROS2の通信にはROS2-For-Unityを用いる
- 自己位置推定にamclを用いるため、環境地図を事前に用意する必要がある
